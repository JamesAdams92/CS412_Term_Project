{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\onurm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\onurm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SVM params: {'C': 1, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best LR params: {'C': 10, 'solver': 'lbfgs'}\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Best NB params: {'alpha': 0.1}\n",
      "\n",
      "Validation Accuracy: 0.6648451730418944\n",
      "\n",
      "Validation Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "                 art       0.36      0.11      0.16        38\n",
      "       entertainment       0.49      0.45      0.47        65\n",
      "             fashion       0.58      0.70      0.64        60\n",
      "                food       0.87      0.91      0.89       102\n",
      "              gaming       0.00      0.00      0.00         3\n",
      "health and lifestyle       0.64      0.75      0.69       100\n",
      "    mom and children       0.63      0.40      0.49        30\n",
      "              sports       0.83      0.65      0.73        23\n",
      "                tech       0.66      0.80      0.72        69\n",
      "              travel       0.65      0.68      0.66        59\n",
      "\n",
      "            accuracy                           0.66       549\n",
      "           macro avg       0.57      0.54      0.55       549\n",
      "        weighted avg       0.65      0.66      0.65       549\n",
      "\n",
      "\n",
      "Full Accuracy: 0.9332360452389639\n",
      "\n",
      "Full Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "                 art       0.98      0.84      0.90       191\n",
      "       entertainment       0.95      0.87      0.91       323\n",
      "             fashion       0.90      0.96      0.93       299\n",
      "                food       0.96      0.98      0.97       511\n",
      "              gaming       1.00      1.00      1.00        13\n",
      "health and lifestyle       0.90      0.93      0.92       502\n",
      "    mom and children       0.97      0.89      0.93       149\n",
      "              sports       1.00      0.93      0.96       113\n",
      "                tech       0.93      0.97      0.95       346\n",
      "              travel       0.89      0.93      0.91       294\n",
      "\n",
      "            accuracy                           0.93      2741\n",
      "           macro avg       0.95      0.93      0.94      2741\n",
      "        weighted avg       0.93      0.93      0.93      2741\n",
      "\n",
      "\n",
      "No ground-truth labels provided for round2, so only predictions are generated.\n",
      "\n",
      "Done! Round2 predictions saved to 'prediction-classification-round3'.\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# 1. All necessary imports\n",
    "###############################################################################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import json\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "turkish_stopwords = stopwords.words('turkish')\n",
    "\n",
    "###############################################################################\n",
    "# 2. Preprocessing function\n",
    "###############################################################################\n",
    "def preprocess_text(text: str):\n",
    "    # Lower casing Turkish text\n",
    "    text = text.casefold()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove special characters (adjust if you want to keep #, @, etc.)\n",
    "    text = re.sub(r'[^a-zçğıöşü0-9\\s#@]', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "###############################################################################\n",
    "# 3. Read and unify the training classification data\n",
    "###############################################################################\n",
    "train_classification_df = pd.read_csv(\"C:/Users/onurm/Desktop/CS412-project/train-classification.csv\")\n",
    "train_classification_df = train_classification_df.rename(\n",
    "    columns={'Unnamed: 0': 'user_id', 'label': 'category'}\n",
    ")\n",
    "train_classification_df[\"category\"] = train_classification_df[\"category\"].apply(str.lower)\n",
    "username2_category = train_classification_df.set_index(\"user_id\").to_dict()[\"category\"]\n",
    "\n",
    "###############################################################################\n",
    "# 4. Read the main JSONL file, separate train vs. unlabeled data\n",
    "###############################################################################\n",
    "train_data_path = \"C:/Users/onurm/Desktop/CS412-project/training-dataset.jsonl.gz\"\n",
    "\n",
    "username2posts_train = {}\n",
    "username2profile_train = {}\n",
    "\n",
    "username2posts_unlabeled = {}\n",
    "username2profile_unlabeled = {}\n",
    "\n",
    "with gzip.open(train_data_path, \"rt\") as fh:\n",
    "    for line in fh:\n",
    "        sample = json.loads(line)\n",
    "        profile = sample[\"profile\"]\n",
    "        username = profile[\"username\"]\n",
    "        if username in username2_category:\n",
    "            username2posts_train[username] = sample[\"posts\"]\n",
    "            username2profile_train[username] = profile\n",
    "        else:\n",
    "            username2posts_unlabeled[username] = sample[\"posts\"]\n",
    "            username2profile_unlabeled[username] = profile\n",
    "\n",
    "###############################################################################\n",
    "#  Additional Feature Extraction\n",
    "###############################################################################\n",
    "def extract_features(username, profile_data, posts_data):\n",
    "    profile = profile_data.get(username, {})\n",
    "    profile_features = {\n",
    "        'follower_count': profile.get('follower_count', 0),\n",
    "        'following_count': profile.get('following_count', 0),\n",
    "        'highlight_reel_count': profile.get('highlight_reel_count', 0),\n",
    "        'is_business': int(profile.get('is_business_account', False)),\n",
    "        'full_name' : profile.get('full_name', ''),\n",
    "        'biography' : profile.get('biography', ''),\n",
    "        'external_url' : profile.get('external_url', ''),\n",
    "        'is_verified' : int(profile.get('is_verified', False)),\n",
    "        'is_private' : int(profile.get('is_private', False)),\n",
    "        'is_business_account' : int(profile.get('is_business_account', False)),\n",
    "        'is_professional_account' : int(profile.get('is_professional_account', False)),\n",
    "        'is_joined_recently' : int(profile.get('is_joined_recently', False)),\n",
    "        'is_business_account' : int(profile.get('is_business_account', False)),\n",
    "        }\n",
    "    posts = posts_data.get(username, [])\n",
    "    if posts:\n",
    "        avg_likes = np.mean([p.get('like_count', 0) or 0 for p in posts])\n",
    "        avg_comments = np.mean([p.get('comments_count', 0) or 0 for p in posts])\n",
    "        media_types = [p.get('media_type', '') for p in posts]\n",
    "        image_ratio = sum(1 for m in media_types if m == 'IMAGE') / len(posts)\n",
    "    else:\n",
    "        avg_likes = 0\n",
    "        avg_comments = 0\n",
    "        image_ratio = 0\n",
    "    \n",
    "    return {\n",
    "        **profile_features,\n",
    "        'avg_likes': avg_likes,\n",
    "        'avg_comments': avg_comments,\n",
    "        'image_ratio': image_ratio\n",
    "    }\n",
    "\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "# 6. Build the \"training\" corpus + label lists\n",
    "###############################################################################\n",
    "all_corpus = []     # TF-IDF text corpus\n",
    "all_labels = []     # categories\n",
    "all_usernames = []  # track usernames in parallel\n",
    "\n",
    "for uname, posts in username2posts_train.items():\n",
    "    captions_clean = []\n",
    "    for p in posts:\n",
    "        cap = p.get(\"caption\", \"\")\n",
    "        if cap:\n",
    "            captions_clean.append(preprocess_text(cap))\n",
    "    joined_text = \"\\n\".join(captions_clean)\n",
    "    all_corpus.append(joined_text)\n",
    "    all_labels.append(username2_category[uname])\n",
    "    all_usernames.append(uname)\n",
    "\n",
    "###############################################################################\n",
    "# 7. Train/Validation Split\n",
    "###############################################################################\n",
    "\n",
    "X_train_corpus, X_val_corpus, y_train, y_val, train_usernames_split, val_usernames_split = train_test_split(\n",
    "    all_corpus,\n",
    "    all_labels,\n",
    "    all_usernames,\n",
    "    test_size=0.2,\n",
    "    stratify=all_labels,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "###############################################################################\n",
    "# 7. Vectorize (TF-IDF) using only the training corpus\n",
    "###############################################################################\n",
    "vectorizer = TfidfVectorizer(stop_words=turkish_stopwords, max_features=5000)\n",
    "vectorizer.fit(X_train_corpus)\n",
    "\n",
    "# Transform\n",
    "x_train_tfidf = vectorizer.transform(X_train_corpus)\n",
    "x_val_tfidf   = vectorizer.transform(X_val_corpus)\n",
    "\n",
    "###############################################################################\n",
    "# 8. Hyperparameter Tuning for SVM\n",
    "###############################################################################\n",
    "svm_clf_base = SVC(probability=True, random_state=42)\n",
    "\n",
    "param_grid_svm = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "svm_grid = GridSearchCV(\n",
    "    svm_clf_base,\n",
    "    param_grid_svm,\n",
    "    scoring='accuracy',\n",
    "    cv=3,  # or 5\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "svm_grid.fit(x_train_tfidf, y_train)\n",
    "\n",
    "print(\"Best SVM params:\", svm_grid.best_params_)\n",
    "best_svm = svm_grid.best_estimator_\n",
    "\n",
    "###############################################################################\n",
    "# 9. Hyperparameter Tuning for LogisticRegression\n",
    "###############################################################################\n",
    "lr_clf_base = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "param_grid_lr = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'solver': ['lbfgs', 'saga'],  # etc.\n",
    "    \n",
    "}\n",
    "\n",
    "lr_grid = GridSearchCV(\n",
    "    lr_clf_base,\n",
    "    param_grid_lr,\n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "lr_grid.fit(x_train_tfidf, y_train)\n",
    "\n",
    "print(\"Best LR params:\", lr_grid.best_params_)\n",
    "best_lr = lr_grid.best_estimator_\n",
    "\n",
    "###############################################################################\n",
    "# 10. Hyperparameter Tuning for Multinomial Naive Bayes\n",
    "###############################################################################\n",
    "nb_clf_base = MultinomialNB()\n",
    "param_grid_nb = {\n",
    "    'alpha': [0.1, 1.0, 5.0]\n",
    "}\n",
    "\n",
    "nb_grid = GridSearchCV(\n",
    "    nb_clf_base,\n",
    "    param_grid_nb,\n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "nb_grid.fit(x_train_tfidf, y_train)\n",
    "\n",
    "print(\"Best NB params:\", nb_grid.best_params_)\n",
    "best_nb = nb_grid.best_estimator_\n",
    "\n",
    "###############################################################################\n",
    "# 11. Build a VotingClassifier with the best models\n",
    "###############################################################################\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('svm', best_svm),\n",
    "        ('lr', best_lr),\n",
    "        ('nb', best_nb)\n",
    "    ],\n",
    "    voting='soft'  \n",
    ")\n",
    "voting_clf.fit(x_train_tfidf, y_train)\n",
    "\n",
    "###############################################################################\n",
    "# 12. Evaluate on the Validation Split\n",
    "###############################################################################\n",
    "y_val_pred = voting_clf.predict(x_val_tfidf)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(\"\\nValidation Accuracy:\", val_accuracy)\n",
    "print(\"\\nValidation Classification Report:\")\n",
    "print(classification_report(y_val, y_val_pred, zero_division=0))\n",
    "\n",
    "###############################################################################\n",
    "# 13. Retrain on all labeled data (train+val) for final model\n",
    "\n",
    "###############################################################################\n",
    "X_full_tfidf = vectorizer.fit_transform(all_corpus)\n",
    "voting_clf.fit(X_full_tfidf, all_labels)\n",
    "y_full_pred = voting_clf.predict(X_full_tfidf)\n",
    "full_accuracy = accuracy_score(all_labels, y_full_pred)\n",
    "print(\"\\nFull Accuracy:\", full_accuracy)\n",
    "print(\"\\nFull Classification Report:\")\n",
    "print(classification_report(all_labels, y_full_pred, zero_division=0))\n",
    "\n",
    "###############################################################################\n",
    "# 14. Evaluate on test-classification-round2.dat\n",
    "###############################################################################\n",
    "test_data_round3_path = \"C:/Users/onurm/Desktop/CS412-project/test-classification-round3.dat\"\n",
    "\n",
    "\n",
    "round2_usernames = []\n",
    "with open(test_data_round3_path, \"rt\") as fh:\n",
    "    for line in fh:\n",
    "        round2_usernames.append(line.strip())\n",
    "\n",
    "\n",
    "\n",
    "ground_truth_round2 = {}  \n",
    "\n",
    "\n",
    "X_round2_corpus = []\n",
    "round2_labels_if_known = []\n",
    "found_usernames = []\n",
    "\n",
    "for uname in round2_usernames:\n",
    "    # Check if in unlabeled set:\n",
    "    if uname in username2posts_unlabeled:\n",
    "        posts = username2posts_unlabeled[uname]\n",
    "    elif uname in username2posts_train:\n",
    "        # Possibly was part of the labeled set\n",
    "        posts = username2posts_train[uname]\n",
    "    else:\n",
    "        # Not found at all => empty\n",
    "        posts = []\n",
    "    \n",
    "    cleaned_captions = []\n",
    "    for p in posts:\n",
    "        cap = p.get(\"caption\", \"\")\n",
    "        if cap:\n",
    "            cleaned_captions.append(preprocess_text(cap))\n",
    "    joined_text = \"\\n\".join(cleaned_captions)\n",
    "    X_round2_corpus.append(joined_text)\n",
    "    found_usernames.append(uname)\n",
    "\n",
    "    # If we do have ground truth for round2\n",
    "    if uname in ground_truth_round2:\n",
    "        round2_labels_if_known.append(ground_truth_round2[uname])\n",
    "    else:\n",
    "        # or an empty string (meaning unknown)\n",
    "        round2_labels_if_known.append(None)\n",
    "\n",
    "# TF-IDF transform\n",
    "X_round2_tfidf = vectorizer.transform(X_round2_corpus)\n",
    "\n",
    "# Predict\n",
    "round2_predictions = voting_clf.predict(X_round2_tfidf)\n",
    "\n",
    "\n",
    "actual_labels = [l for l in round2_labels_if_known if l is not None]\n",
    "if len(actual_labels) == len(round2_labels_if_known) and len(actual_labels) > 0:\n",
    "    \n",
    "    round2_accuracy = accuracy_score(round2_labels_if_known, round2_predictions)\n",
    "    print(\"\\nROUND3 Accuracy:\", round2_accuracy)\n",
    "    print(\"\\nROUND3 Classification Report:\")\n",
    "    print(classification_report(round2_labels_if_known, round2_predictions, zero_division=0))\n",
    "else:\n",
    "    print(\"\\nNo ground-truth labels provided for round2, so only predictions are generated.\")\n",
    "\n",
    "\n",
    "round2_output = {u: p for u, p in zip(found_usernames, round2_predictions)}\n",
    "with open(\"prediction-classification-round3\", \"w\", encoding='utf-8') as of:\n",
    "    json.dump(round2_output, of, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"\\nDone! Round2 predictions saved to 'prediction-classification-round3'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
